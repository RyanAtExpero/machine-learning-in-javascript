{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "1d31ea07-f8f0-47cc-9fc0-0f37352e5cf8"
    }
   },
   "source": [
    "#### Objectives\n",
    "\n",
    "By the end of this notebook you will know:\n",
    "\n",
    " - what is unsupervised learning\n",
    " - how to run keman clustering\n",
    " - hot to perform minimum distance classification\n",
    " - be better at plotting\n",
    " - compute confidence\n",
    " - compute a confusion matrix and derive fomr key metrics\n",
    " - plot and interpret a confusion matrix \n",
    " - the importance and impact of initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e462b3b0-f88e-4099-90ee-901df0654a3e"
    }
   },
   "outputs": [],
   "source": [
    "// boring imports\n",
    "var {loadUnlabelledWine, grid2} = require('./utils')\n",
    "var Plot = require('plotly-notebook-js');\n",
    "var table = require('text-table');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "![Unsupervised Learning slide](images/slide_unsupervised.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3059fd90-8176-4b05-b339-e531e4199fb6"
    }
   },
   "source": [
    "# K Means Clustering\n",
    "\n",
    "\n",
    "![means algorihtm](images/slide_kmeans.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ba8551b1-dbe2-452b-bf0b-bbc686671ae4"
    }
   },
   "source": [
    "### Setup\n",
    "\n",
    "Load our dataset and pick out the two features of interest, the ones we were looking at on the previous page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "95d0c075-1026-415b-97c4-43666b99424c"
    }
   },
   "outputs": [],
   "source": [
    "var {features, dataset} = loadUnlabelledWine({ verbose: true });"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "ee7b4161-ca1f-47e6-a88c-c2a431c8b1c0"
    }
   },
   "outputs": [],
   "source": [
    "var input = dataset.map(d => [ d[0], d[11] ]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "35953a86-a704-4db8-8ce4-f007cb2f309b"
    }
   },
   "source": [
    "### Clustering\n",
    "\n",
    "Now run the algorithm, check out the docs [here](https://mljs.github.io/kmeans/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e5761634-7895-4e47-bc87-2ecc15ec675f"
    }
   },
   "outputs": [],
   "source": [
    "var KMEANS = require('ml-kmeans');\n",
    "\n",
    "var K = 3;\n",
    "\n",
    "var options = {\n",
    "    maxIterations: 100,\n",
    "    tolerance: 1e-6,\n",
    "    withIterations: false,\n",
    "    // distanceFunction: () => {}, // can specify our own distance but may not converge\n",
    "    initialization: 'random'\n",
    "}\n",
    "\n",
    "var ans = KMEANS(input, 3, options)\n",
    "\n",
    "var {converged, clusters, centroids, iterations} = ans;\n",
    "\n",
    "if (converged) {\n",
    "    console.log(\"Converged after\", iterations, \"iterations\")\n",
    "}\n",
    "else {\n",
    "    console.log(\"Did not converge after\", iterations, \"iterations\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "4f626a26-718a-4aad-a20a-5327bc6b6702"
    }
   },
   "source": [
    "### Plot the results\n",
    "\n",
    "We now have class labels in `ans.clusters` corresponding to each feature vector in our `input` array. Let's scatterplot the results but color these by class label.\n",
    "\n",
    "#### TODO: now add tha class centroids to the plot with larger markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "ee539d2d-10e7-4a85-94e6-30818937ed4e"
    }
   },
   "outputs": [],
   "source": [
    "var trace = { \n",
    "    x: input.map(d => d[0]),\n",
    "    y: input.map(d => d[1]),\n",
    "    mode: 'markers',\n",
    "    marker: { \n",
    "        color: clusters, // <- here are our results\n",
    "        size: 8,\n",
    "        colorbar: {\n",
    "            xpad: 100\n",
    "        }\n",
    "    },\n",
    "    type: 'scatter'\n",
    "};\n",
    "\n",
    "var centroidsTraces = centroids.map(d => ({\n",
    "    x: [d.centroid[0]], y: [d.centroid[1]],\n",
    "    mode: 'markers',\n",
    "    marker: { \n",
    "        size: 20,\n",
    "        line: { width: 2, color: '#000' },\n",
    "        opacity: 1\n",
    "    },\n",
    "    opacity: 0.3,\n",
    "    type: 'scatter',\n",
    "}));\n",
    "\n",
    "var layout = { width: 800, height: 700, xaxis: { title: features[0] }, yaxis: { title: features[11] }};\n",
    "\n",
    "$$html$$ = Plot.createPlot([trace, ...centroidsTraces], layout).render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "a78a9c7c-be95-4b2e-a6bc-d670cf7ffae1"
    }
   },
   "source": [
    "### Plot the feature space partitioning\n",
    "\n",
    "Now let's look at the decision boundaries in the feature space. \n",
    "\n",
    "Kmeans already labelled our training set for us but if we need to determine the class of a datapoint that we have not seen yet when we need ot use a classifer with the `centroids` that kmeans gave us.\n",
    "\n",
    "This is known as a `forward pass`. KMeans has used the training set to *learn* the k class centroids, but to *generaise* to data we have not seen before we need to run a classifier.\n",
    "\n",
    "Luckily we just created one in the last notebook. Either copy the function definition in here or create a new `.js` file, and `require` that (if you do, remember to restart the kernel otherwise the notebook won't see the new file).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e8a81df5-7c2b-4c64-822c-a6b41adb62fb"
    }
   },
   "outputs": [],
   "source": [
    "function myClassifier(centroids, inputs) {\n",
    "    throw new Error(\"You need to implement this\")\n",
    "}\n",
    "\n",
    "var g = grid2();\n",
    "var H = myClassifier(centroids, input); \n",
    "\n",
    "var trace = {\n",
    "    z: H,\n",
    "    type: \"heatmap\"\n",
    "}\n",
    "\n",
    "var layout = {\n",
    "    title: \"damped circular wavefront\",\n",
    "    width: 700,\n",
    "    height: 700\n",
    "}\n",
    "\n",
    "$$html$$ = Plot.createPlot([trace], layout).render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "01bc6ea8-219c-44a7-8264-dfc9ba2350d3"
    }
   },
   "source": [
    " ### Time to Play around\n",
    " \n",
    " Time to try a few different things out and see the effect on the clustering and classification\n",
    " \n",
    " - try with different initialisations\n",
    "   - mostDistant\n",
    "   - use the class centers that we picked manually\n",
    " - try with different values of K, what happens? why?\n",
    " - try with fewer points, how does restricting the training set affect class positions?\n",
    " - (time allowing) try with a different dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fb3431c5-9867-4f54-91c6-05575c3341f9"
    }
   },
   "source": [
    "### Confidence\n",
    "\n",
    "So we have assigned all of the samples in our training set to one of K classes and we've used the centroids produced to configure a classifier, so we can classify wines that we've never seen before, we've genrelised!\n",
    "\n",
    "\n",
    "But are all wines in a class equal? how can we measure confidence of membership in any one class? (hint: look back at out scatter plot)\n",
    "\n",
    "\n",
    "##### TODO Think of a confidence measure, compute it and display a new scatter plot with marker sizes adjusted for confidence\n",
    "\n",
    "So for each of our N entires in `input` and `clusters`, we'll want a new N entry list `confidence`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "1ad289bd-d5f3-452e-bf7f-1258cbdaa3a4"
    }
   },
   "outputs": [],
   "source": [
    "// derive a confidence measure and compute it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e52675fc-d610-4c30-b160-978bc92832eb"
    }
   },
   "outputs": [],
   "source": [
    "// grabs the scatter plot code form above and customise it to some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "12414fc4-be9c-4349-b1e2-15b8f6029d73"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d68070d3-5c5e-48ef-83bf-ae4ae2267562"
    }
   },
   "source": [
    "### Accuracy\n",
    "\n",
    "Ok, we have classified some data and calculated how confident we are with each of the classifications. \n",
    "\n",
    "But how do we know whether our classification is right? We'll in many applications of unsupervised learning, we don't know for sure as we typically use this type of technique when we don't know the expected outputs before hand. \n",
    "\n",
    "However, in this example dataset, we do have training labels available but we've just not loaded them. So let's reload those and measure the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "b2085534-e8f0-46dd-8b68-e03c2043f76d"
    }
   },
   "outputs": [],
   "source": [
    "var {loadLabelledWine} = require('./utils')\n",
    "var labelledDataset = loadLabelledWine({ verbose: true }).dataset;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d8218bf7-cb02-400d-976e-c7976ae562c7"
    }
   },
   "source": [
    "Notice the additional (last) column containing the known class index. We need to note that they are coulding forom `1`, whilst our class labels are from `0`, we'll need to compensate for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "de004a34-a3f6-4ead-90b5-da8d1ab8413a"
    }
   },
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "With mljs we can use the confusion matrix package to compute a suite of different measures to determine the performance on our clustering and classifer.\n",
    "\n",
    "Dig into [the docs](https://mljs.github.io/confusion-matrix/) and compute:\n",
    "\n",
    " - overall `accuracy`\n",
    " - the `F1 score` for each class label. \n",
    " - the average `F! score`\n",
    " \n",
    "These metrics are in the range [0,1] or [appaling, 100% match]. print them out on the console.\n",
    "\n",
    "NB: with this library we can compute a full suite of diagnisoc measures, see the table [here](https://en.wikipedia.org/wiki/F1_score#Diagnostic_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var ConfusionMatrix = require('ml-confusion-matrix');\n",
    "var actuals = labelledDataset.map(d => d[13]-1);\n",
    "var predicted = clusters.map(d => d);\n",
    "\n",
    "var C = ConfusionMatrix.fromLabels(actuals, predicted)\n",
    "\n",
    "// compute and print out some metrics here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So accuracy might not have been as good as we hoped for.\n",
    "\n",
    "Why? let's dig deeper and look at the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d54579fb-5451-403f-a9eb-dd3a3d5e308a"
    }
   },
   "outputs": [],
   "source": [
    "var M = C.getMatrix();\n",
    "\n",
    "var trace = { \n",
    "    x: [0,1,2],\n",
    "    y: [0,1,2],\n",
    "    z: M,\n",
    "    type: 'heatmap',\n",
    "    showscale: false,\n",
    "    colorscale:[[0, '#3D9970'], [100, '#001f3f']]\n",
    "};\n",
    "\n",
    "var annotations = [];\n",
    "\n",
    "M.map((a,y) => {\n",
    "    a.map((b,x) => {\n",
    "        annotations.push(\n",
    "            {\n",
    "                x: x,\n",
    "                y: y,\n",
    "                text: M[y][x],\n",
    "                font: {\n",
    "                    family: 'Arial',\n",
    "                    size: 12,\n",
    "                    color: 'white'\n",
    "                  },\n",
    "                showarrow: false\n",
    "            }\n",
    "        )\n",
    "    })\n",
    "})\n",
    "\n",
    "var layout = { \n",
    "    xaxis: { title: \"predicted\", side: 'top' },\n",
    "    yaxis: { title: \"actuals\", nticks: 6, autosize: false, autorange: 'reversed' },\n",
    "    annotations,\n",
    "    width: 500, height: 500};\n",
    "\n",
    "$$html$$ = Plot.createPlot([trace], layout).render();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the original labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var trace = { \n",
    "    x: input.map(d => d[0]),\n",
    "    y: input.map(d => d[1]),\n",
    "    mode: 'markers',\n",
    "    marker: { \n",
    "        color: actuals, // <- here are the true labels\n",
    "        size: 8,\n",
    "        colorbar: {\n",
    "            xpad: 100\n",
    "        }\n",
    "    },\n",
    "    type: 'scatter'\n",
    "};\n",
    "\n",
    "var layout = { width: 800, height: 700, xaxis: { title: features[0] }, yaxis: { title: features[11] }};\n",
    "\n",
    "$$html$$ = Plot.createPlot([trace, ...centroidsTraces], layout).render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the attributes you chose, we can see some gross misclassifications here. In high accuracy cases the main diagonal contains the most weight.\n",
    "\n",
    "So why do you thing this has done so poorly? (if it has)\n",
    "\n",
    "Try running the whole notebook again (Cell > Run All), a few times and watch the score and the confusion matrix, does it change? \n",
    "\n",
    "Any idea what is happening?\n",
    "\n",
    "Any idea how to fix it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "72409247-a808-4f09-9310-78163fdb78f0"
    }
   },
   "source": [
    "#### Further Reading\n",
    "\n",
    "More sophisticated techniques can produce different\n",
    "\n",
    " - bayes learning and 2nd order bayes classifers\n",
    " - gaussian mixture modelling\n",
    " - measuring performance in undersuipervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "jp-Babel (Node.js)",
   "language": "babel",
   "name": "babel"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "8.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
